---
title: "Sales Prediction Exercise"
author: "Tamas Koncz"
date: '2018-03-04'
output:
  html_document:
    df_print: paged
    toc: true
    number_sections: true
    theme: united
  html_notebook:
    df_print: paged
    toc: true
    number_sections: true
    theme: united
---

```{r setup, message=FALSE, include=FALSE}
library(data.table)
library(dplyr)
library(purrr)
library(lubridate)
library(ggplot2)
library(gridExtra)
library(scales)
library(reshape)

library(caret)
library(glmnet)

library(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

options(digits = 4)
options(scipen = 999)

theme_set(theme_minimal())   # globally set ggplot theme

set.seed(93)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
```

```{r, echo=FALSE}
fun_count_na <- function(dt) {
  ##counts and reports the missing observations for each column in a data.table object
  missing_values <- as.data.table(t(dt[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dt)]),
                                  keep.rownames = TRUE)
  setnames(missing_values, c("variable", "NA Count"))
  
  return(missing_values[order(-`NA Count`)])
}
```

# Purpose definition

The goal of this exercise is to predict sales for the coming year for different customers (groups of customers). I'll use standard prediction techniques and practices to create predictive models, while explaining the decision I've made along the code as well.  

The output will be available in an .Rmd and .html files.  
My documentation approach is the following: the .html output will show only limited amount of code, only snippets that are helpful for understanding the logic applied (others are "hidden" by the 'echo = FALSE' prefix for code chunks).  
For the full code base, please refer to the .Rmd file.  
  
The presentation of the analysis is built of the following parts:  
    1. Data review & cleaning  
    2. Exploratory data analysis  
    3. Featuring engineering  
    4. Model & model selection
    5. Performance evaluation  
    
An important feature of data analysis is that it is an iterative process - we are likely to go back to earlier steps once we learn something new about the data to expand what we did.  
This iterative process is by definition not very linear - presenting things as they evolved would be a very messy read for anyone.  
Hence, this document is created to capture the final state of the analysis - many of these smaller iterations are inherently lost unfortunately. Please keep this in mind when reading.  
  

# Data review & cleaning  

After reading in the data, let's take a glimpse at the data structure we'll be working with:
```{r, echo = FALSE}
data <- fread("./../Data/Sales Prediction/training.csv", stringsAsFactors = F)
glimpse(data)
head(data)
```  
  
One thing to notice is that we have a field containing dates, however R treats it as a character column by default. 
  
We'll correct that in the next step, but before that let's check if there is any observation that will require some fixing as well:

```{r, echo=FALSE}
fun_count_na(data)
```
  
No NAs, that's good news. Before deciding whether the dataset is complete or not, let's make sure all the sales & quantity values make sense, by checking how many 0-s we encounter.  
  
First, for quantities:  
```{r}
data %>%
  filter(quantity == 0) %>%
  summarize(count = n(),
            sum_sales_amount = sum(sales_amount))
```  
This looks very limited. Without further context on the data, I'll just assume they are recording errors, and exlcude them for the dataset.  

```{r}
data %>%
  filter(quantity != 0) %>%
  filter(sales_amount == 0) %>%
  summarize(count = n(),
            sum_quantity = sum(quantity))
```

We are encountering many (6947 in total) observations for which sales_amount is 0. Interestingly, sometimes even the record quantity is larger than 1 for these.   
Although they are marginal compared to the number of all observations (500K+), a decision needs to be still made how to handle them, as they are likely misleading for any model in their current format.  

```{r, include = FALSE}
data %>%
  filter(product_id == 1533517) %>%
  head()
```  
  
There might be a reason behind having 0 values (one I can think of is they are being part of some kind of promotion), and this reason could actually be indicative of future sales (e.g. the promotion is for frequent shoppers etc.).   
In this case, features should be created out of these items to help enhance the model's prediction.  
On the other hand, they might just be simple data issues. Deleted items still showing up as line items, incorrect prices (hence sales_amount), or anything similar.  
Right now we can simply and assume they actually represent wrong data - so by taking the easy path, I simply exclude them from further modeling efforts.  
  
Applying the fixes before we continue:  
```{r}
data <- data %>%
          filter(sales_amount > 0) %>% 
          mutate(purchase_date = as.Date(purchase_date, "%Y-%m-%d")) %>%
          arrange(contact_id, purchase_date) %>%
          select(contact_id, order_id, purchase_date, product_id, quantity, sales_amount)
```

# Data exploration  
  
As standard practice, I'll continue by visually exploring what's in the data, that might be useful for sales prediction.  
  
My starting hypothesis is that one of the most important features to understand is whether we are working returning customers - which materializes as having more than one purchase_date per contact_id (the other is the seasonality effect - more on that later).  
  
The below table gives us a brief look into customers who are generating the most number of transactions:
```{r, echo = FALSE}
per_contact_avgs <- data %>%
                      group_by(contact_id) %>%
                      summarize(total_line_items = n(),
                                transaction_count = n_distinct(order_id),
                                unique_products = n_distinct(product_id),
                                avg_quantity = mean(quantity),
                                median_quantity = median(quantity),
                                avg_sales_amount = mean(sales_amount),
                                median_sales_amount = median(sales_amount),
                                total_sales_amount = sum(sales_amount))
per_contact_avgs %>%
  arrange(desc(transaction_count)) %>%
  head(10)
```
  
Below is a visualization of the distribution of transactions among customers, broken into two groups, one having more than 25 transactions, and one having less.  
```{r, fig.width=8, fig.height=3, fig.align='center'}
##visualize
per_contact_avgs %>%
  mutate(transaction_category = ifelse(transaction_count < 25, 
                                       "Less than 25 transactions",
                                       "25+ transactions")) %>%
  mutate(transaction_category = factor(transaction_category, levels =
                                         c("Less than 25 transactions", "25+ transactions"))) %>%
  ggplot() + 
  geom_histogram(aes(x= transaction_count, fill = transaction_category)) +
  facet_wrap(~transaction_category, scales = "free") +
  scale_y_continuous(name="# of contact_id-s", labels = comma) +
  scale_x_continuous(name="Count of total transactions", labels = comma) +
  labs(title = "Grouping contact_id-s based on number of transactions")
```

What do we learn from this?  
    1. First, most customers only buy once. This is important for our modeling, as we won't know anything about them (priori to their first purchase) when we try to forecast their sales. Hence, all we'll be able to rely on is some generalizations made based on when they make their purchase.  
    2. Any way we slice the customers, transaction counts will be heavily skewed to the left - even more than for lognormal distributions.  
    3. A very important thing that we _don't_ learn is how much sales these different groups generate.  
    
Maybe not suprisingly, total sales per contract_id do closely resemble a lognormal distribution:
```{r, echo=FALSE, fig.width=10, fig.height=6, fig.align='center'}
p1 <- per_contact_avgs %>%
  mutate(transaction_category = ifelse(transaction_count < 2, 
                                       "One-time customer",
                                       "Returning customer")) %>%
  mutate(transaction_category = factor(transaction_category, levels =
                                         c("One-time customer", "Returning customer"))) %>%
  ggplot() + 
  geom_histogram(aes(x= total_sales_amount, fill = transaction_category)) +
  scale_x_continuous(trans='log10', labels = comma) +
  facet_wrap(~transaction_category, scales = "free")


temp <- per_contact_avgs %>%
  mutate(transaction_category = ifelse(transaction_count < 2, 
                                       "One-time customer",
                                       "Returning customer")) %>%
  mutate(transaction_category = factor(transaction_category, levels =
                                         c("One-time customer", "Returning customer")))
temp1 <- temp %>%
          filter(transaction_category == "One-time customer")

temp2 <- temp %>%
          filter(transaction_category == "Returning customer")


p2 <- ggplot() + 
  geom_density(data = temp1, aes(x= avg_sales_amount, fill = transaction_category), alpha = 0.4) +
  geom_density(data = temp2, aes(x= avg_sales_amount, fill = transaction_category), alpha = 0.25) +
  scale_x_continuous(trans='log10', labels = comma) 


grid.arrange(p1, p2, ncol = 1)
```

## bit further text on this is needed... specially the avg. sales part


The second important factor to understand is how seasonality (basically time of purchase) is related to sales amounts.

```{r, echo = FALSE}
ordered_year_month <- data %>%
                        mutate(purchase_year = year(purchase_date)) %>%
                        mutate(purchase_month = month(purchase_date)) %>%
                        distinct(purchase_year, purchase_month) %>%
                        mutate(purchase_year_month = paste(as.character(purchase_year),
                                                           as.character(purchase_month), sep = "-")) %>%
                        arrange(purchase_year, purchase_month) %>%
                        select(purchase_year_month)

data <- data %>%
  mutate(purchase_year = year(purchase_date)) %>%
  mutate(purchase_month = month(purchase_date)) %>%
  mutate(purchase_year_month = paste(as.character(purchase_year),
                                     as.character(purchase_month), sep = "-")) %>%
  mutate(purchase_year_month = factor(purchase_year_month, 
                                      levels = ordered_year_month[["purchase_year_month"]])) 

```

```{r, echo = FALSE}
fun_plot_trend <- function(df1, x_var, y_var, x_breaks) {
  
  p <- ggplot(data = df1, aes_string(x= x_var)) + 
    geom_point(aes_string(y = y_var)) +
    geom_line(aes_string(y = y_var), group=1) +
    scale_x_discrete(breaks = x_breaks)  
    
  return(p)
}
```

```{r, echo = FALSE}
month_breaks <- ordered_year_month$purchase_year_month[seq(0, 24, 3)]

p1 <- data %>%
  group_by(purchase_year_month) %>%
  summarize(sales = sum(sales_amount)) %>%
  fun_plot_trend(x_var = "purchase_year_month", 
                 y_var = "sales", x_breaks = month_breaks)
  
  
p2 <- data %>%
  group_by(purchase_year_month) %>%
  summarize(transaction_count = n_distinct(order_id)) %>%
  fun_plot_trend(x_var = "purchase_year_month", 
                 y_var = "transaction_count", x_breaks = month_breaks)

p3 <- data %>%
  group_by(purchase_year_month, order_id) %>%
  summarize(sales_amount = sum(sales_amount)) %>%
  group_by(purchase_year_month) %>%
  summarize(avg_sales_amount = mean(sales_amount)) %>%
  fun_plot_trend(x_var = "purchase_year_month", 
                 y_var = "avg_sales_amount", x_breaks = month_breaks)

grid.arrange(p1, p2, p3, ncol = 1)

##todo tidy-up, explain
```

```{r}
data %>%
  arrange(contact_id, purchase_date) %>%
  head(20)
```
```{r}
#TODO: break this out by purchase count 0 or 1+ by contact_id
data %>%
  group_by(purchase_year, purchase_month, contact_id, order_id) %>%
  summarize(sales_amount = sum(sales_amount)) %>%
  group_by(purchase_year, purchase_month, contact_id) %>%
  summarize(avg_sales_amount = mean(sales_amount)) %>%
  group_by(purchase_year, purchase_month) %>%
  summarize(avg_sales_amount = mean(avg_sales_amount)) %>%
  ggplot(aes(x = as.factor(purchase_month), y = avg_sales_amount)) +
    geom_bar(stat="identity") + 
    scale_x_discrete(breaks = c(1:12)) +
    facet_grid(~purchase_year)
```

# Feature engineering  

In this section, we'll be getting the data in shape to use for prediction.  
  
In this exercise, the goal is to predict a customers spending for a new year, based on historical spending patterns.   
We have two years in our dataset, 2012 & 2013 - given this, the setup will be rather simple - using everything we can from 2012, we want to predict a total sales_amount number for 2013 for a given customer.  
When working with time data, and trying to make predictions, it's very important to avoid any "future leaks" - we only want to predict based on information that was available _a priori_ compared to when we are making the prediction.  
Here, we basically already achieve this by data from 2012 as predictors for 2013 sales, so this is not something we need to worry about going forward.  
  
  

```{r, echo = FALSE}
data <- data %>%
          mutate(year = year(purchase_date))
```

So, first steps first: let's calculate the per customer total spending for 2013.  
```{r}
sales_2013 <- data %>%
                filter(year == 2013) %>%
                group_by(contact_id) %>%
                summarize(total_sales_2013 = sum(sales_amount, na.rm = TRUE))
```
  
  
  
A look at how the data looks arranged by top spenders on top:  
```{r, echo = FALSE}
sales_2013 %>%
  arrange(desc(total_sales_2013), contact_id) %>%
  head(5)
```
  
  
  
For 2012 we can start by building a couple of features that are rather "obvious" - total sales, transaction count, etc.:

```{r}
data_2012 <- data %>%
              filter(year == 2012)

features_2012 <- data_2012 %>%
                  group_by(contact_id) %>%
                  summarize(total_sales_2012 = sum(sales_amount, na.rm = T),
                            total_line_items_2012 = n(),
                            total_purchases_2012 = n_distinct(order_id),
                            total_quantity_2012 = sum(quantity, na.rm = T),
                            biggest_purchase_item = max(sales_amount, na.rm = T),
                            smallest_purchase_item = min(sales_amount, na.rm = T)
                            ) %>%
                  mutate(existing_2012_contact_id = "Y")
```
  
  
Again, look at how the data looks arranged by top spenders on top:  
```{r}
features_2012 %>%
  arrange(desc(total_sales_2012), contact_id) %>%
  head(5)
```
  
The results are already promising - some customers are actually getting repeated in the top5 (e.g. contact_id 35244243 or 49141897), which is just a minor signal that there is likely to be a relationship between the two years' spending patterns. I am going to explore this and a lot more futher after we are finished building all our features for prediction.

More features on yearly data:
```{r}
features_2_2012 <- data_2012 %>%
                    group_by(contact_id, order_id) %>%
                    summarize(sales_amount = sum(sales_amount),
                              most_expensive_line = max(sales_amount),
                              least_expensive_line = min(sales_amount),
                              count_lines = n()) %>%
                    group_by(contact_id) %>%
                    summarize(avg_sales_by_order_2012 = mean(sales_amount),
                              max_sales_by_order_2012 = max(sales_amount),
                              min_sales_by_order_2012 = min(sales_amount),
                              avg_most_expensive_line_by_order_2012 = mean(most_expensive_line),
                              avg_least_expensive_line_by_order_2012 = mean(least_expensive_line),
                              avg_line_item_count_by_order_2012 = mean(count_lines),
                              purchase_count = n())
```
  
  
A lot of features were created already - however, all of these used the data for the whole year.  
We've seen that seasonality is important - let's build features that take this into account in some way:
```{r}
features_3_2012 <- data_2012 %>%
                    group_by(contact_id, purchase_month) %>%
                    summarize(sales_amount = sum(sales_amount)) %>%
                    group_by(contact_id) %>%
                    tidyr::spread(key = purchase_month, value = sales_amount, fill = 0, sep= "_")

new_feature_names <- paste(gsub("purchase_month", "sales_in_month", names(features_3_2012)), "2012", sep = "_")[2:13]
setnames(features_3_2012, old = names(features_3_2012)[2:13], new_feature_names)

features_4_2012 <- data_2012 %>%
                    group_by(contact_id, purchase_month) %>%
                    summarize(order_count = n_distinct(order_id)) %>%
                    group_by(contact_id) %>%
                    tidyr::spread(key = purchase_month, value = order_count, fill = 0, sep= "_")

new_feature_names <- paste(gsub("purchase_month", "orders_in_month", names(features_4_2012)), "2012", sep = "_")[2:13]
setnames(features_4_2012, old = names(features_4_2012)[2:13], new_feature_names)
```
  
  
Putting together what we have:
```{r}
df_for_prediction <- sales_2013 %>%
                      left_join(features_2012, by = "contact_id") %>%
                      left_join(features_2_2012, by = "contact_id") %>%
                      left_join(features_3_2012, by = "contact_id") %>%
                      left_join(features_4_2012, by = "contact_id")
```
  
Taking a look:  
```{r}
head(df_for_prediction)
```

Not suprisingly, the features have many NAs:
```{r, echo = FALSE}
fun_count_na(data.table(df_for_prediction))
```



What does this mean? Remember, many of the customers are one-off purchasers, for whom no historical data will be available.  
Here (based on the assumption that the starting dataset was a complete), we know that all the NAs are actually true 0s, hence we can just substitue the NAs with 0s.  
  
```{r}
df_for_prediction <- df_for_prediction %>%
                      mutate(existing_2012_contact_id = ifelse(!is.na(existing_2012_contact_id), "Yes", "No"))

df_for_prediction <- map(df_for_prediction, function(x) ifelse(is.na(x), 0, x)) %>% ##given vector x, if a value is NA it will be replaced by 0
                      as.data.table()
## Notes: this approach needs to be applied with caution! we don't want to replace non-number NAs with 0 for example.
## In this case it will be fine though - we already understand where NAs happen in the data


head(df_for_prediction)
```  
    
  
Making predictions on the zeros is one way to handle this issue - another one would be two build separate models for returning customers (a more complex one), and new customers (something very simple).  
We'll see in the modeling phase which one makes more sense in this case.  

## Visual exploration of the "new" features  

```{r}
names(df_for_prediction)
```

```{r, echo = FALSE, fig.width=10, fig.height=6, fig.align='center'}
p1 <- df_for_prediction %>%
  ggplot(aes(x= total_sales_2012, y= total_sales_2013)) +
    geom_point(alpha = 0.1) +
    geom_smooth()

p2 <- df_for_prediction %>%
  ggplot(aes(x= total_line_items_2012, y= total_sales_2013)) +
    geom_point(alpha = 0.1) +
    geom_smooth()

p3 <- df_for_prediction %>%
  ggplot(aes(x= total_purchases_2012, y= total_sales_2013)) +
    geom_point(alpha = 0.1) +
    geom_smooth()

p4 <- df_for_prediction %>%
  ggplot(aes(x= total_quantity_2012, y= total_sales_2013)) +
    geom_point(alpha = 0.1) +
    geom_smooth()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```



# Modeling & model selection  

For this part I'll leverage the functionality given by Max Kuhn's caret package, which gives easy access to many predictive methods, with possibility to tune the most important parameters.  
  
Briefly, this step is of three parts: I'll train & tune models with cross-validation.  
Once this is done, we can move onto selecting the best model - I'll use a separate held-out set for this, to avoid measuring performance on the tuning set.  
Selection criteria will be simply RMSE, which I'll try to optimize not just in general, but for separte sets of customer types as well.  

## Separating training and evaluation datasets  
  
Let's separate the different datasets we will use for the different steps. Caret helps us with this one as well, hence we don't need to hustle with randomization.  
( _Note: At this point we would need to be cautious with time-series data and partitioning. However, the way the dataset was engineered, this is no longer a concern._)
```{r}
training_ratio <- 0.6

set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = df_for_prediction[["total_sales_2013"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- df_for_prediction[train_indices, ]
data_test <- df_for_prediction[-train_indices, ]

split_ratio <- 3/4

set.seed(93) #for reproducibility
perf_indices <- createDataPartition(y = data_test[["total_sales_2013"]],
                                     times = 1,
                                     p = split_ratio,
                                     list = FALSE)
data_test <- data_test[perf_indices, ]
data_perf <- data_test[-perf_indices, ]
```
  
We'll be using 60% of the all observations for training&tunining, 30% for model selection, and a third set of 10% for the final evaulation of the best model's performance.  
  
## Training & parameter tuning  

When building models, I think using benchmarks is a good practice.  
Generally, I would break benchmarks into two categories:  
    1. Base models, which can be used to evaulate the added value by more complex ones
    2. "The best possible model", usually something very complex for prediction. This can be later stripped of the extra features, and see if a simpler version (which might be easier to interpret) is able to provide similar performance metrics
    
We'll build one from the 1st category - using only one predictor, total sales from 2012. This model will be a simple linear model (we've seen it in the previous visuals that the relationship is close to linear).  
  
As discussed, for all models I'm using 10-fold CV to help avoid overfitting:  
```{r}
train_control <- trainControl(method = "cv",
                              number = 10)
```

```{r}
data_train_full <- copy(data_train)
data_train_just_returning <- data_train %>%
                                filter(existing_2012_contact_id == "Yes") %>%
                                select(everything(), -existing_2012_contact_id)

avg_sales_2012 <- mean(df_for_prediction$total_sales_2012)
```
### Benchmark model - simple glm  
  
Training the model:
```{r}
#base model
set.seed(93)
glm_fit_full <- train(total_sales_2013 ~ total_sales_2012,
                       method = "glm",
                       data = data_train_full,
                       trControl = train_control)

glm_fit_just_returning <- train(total_sales_2013 ~ total_sales_2012,
                                 method = "glm",
                                 data = data_train_just_returning,
                                 trControl = train_control)
```

```{r}
glm_full_predictions <- predict.train(glm_fit_full, newdata = data_test)

glm_just_returning_predictions <- predict.train(glm_fit_just_returning, newdata = data_test) %>%
                                    bind_cols(predicted_sales_w_corr = ., existing_2012_contact_id = data_test$existing_2012_contact_id, actual_sales = data_test$total_sales_2013) %>%
                                    mutate(predicted_sales_w_corr = ifelse(existing_2012_contact_id == "Yes", predicted_sales_w_corr, avg_sales_2012))


actual_vs_pred <- bind_cols(predicted = glm_full_predictions, glm_just_returning_predictions) %>% 
                    as.data.table()


rmse_full <- RMSE(data_test$total_sales_2013, 
                  actual_vs_pred$predicted)
rmse_just_returning <- RMSE(data_test[existing_2012_contact_id == "Yes"]$total_sales_2013,
                            actual_vs_pred[existing_2012_contact_id == "Yes"]$predicted)

rmse_corr_full <- RMSE(data_test$total_sales_2013,
                       actual_vs_pred$predicted_sales_w_corr)
rmse_corr_just_returning <- RMSE(data_test[existing_2012_contact_id == "Yes"]$total_sales_2013,
                                 actual_vs_pred[existing_2012_contact_id == "Yes"]$predicted_sales_w_corr)


rmse_full
rmse_just_returning
rmse_corr_full
rmse_corr_just_returning


calibration <- actual_vs_pred %>%
                    mutate(category = cut(actual_sales,
                                          c(0, 10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000, Inf),
                                          include.lowest = TRUE)) %>%
                    group_by(category) %>%
                    summarize(mean_actual = mean(actual_sales),
                              mean_predicted = mean(predicted),
                              mean_predicted_w_corr = mean(predicted_sales_w_corr),
                              num_obs = n())

calibration %>%
  ggplot(aes(x= mean_actual)) +
    geom_point(aes(y = mean_predicted, size= num_obs)) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    ylim(0, 12000) + xlim(0, 12000) +
    scale_y_continuous(name = "Avg. predicted sales", labels = comma, trans="log10", breaks = c(0, 500, 1000, 5000, 10000)) +
    scale_x_continuous(name = "Avg. actual sales", labels = comma, trans="log10", breaks = c(0, 500, 1000, 5000, 10000)) 
```


### Model 2 - glmnet with LASSO & ridge - parameter tuned


```{r}
tune_grid <- expand.grid("alpha" = c(0, 1),
                         "lambda" = seq(from = 0.00, to = 1, by = 0.025)) ##this was pre-tuned to select best set of parameters to try

set.seed(93)
glmnet_fit <- train(total_sales_2013 ~ . -contact_id,
                   method = "glmnet",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = tune_grid,
                   preProcess = c("center", "scale"))
```


```{r}
glmnet_predictions <- predict.train(glmnet_fit, newdata = data_test)
RMSE(data_test$total_sales_2013, glmnet_predictions)
```

### Model 3 - single decision tree  

```{r}
set.seed(93)
rpart_fit <- train(total_sales_2013 ~ . -contact_id,
                   data = data_train,
                   method = "rpart",
                   tuneLength = 20,
                   trControl = train_control)
```

```{r}
rpart_predictions <- predict.train(rpart_fit, newdata = data_test)
RMSE(data_test$total_sales_2013, rpart_predictions)
```


### Model 4 - bagged tree or RF

```{r}
set.seed(93)
treebag_fit <- train(total_sales_2013 ~ . -contact_id,
                   data = data_train,
                   method = "treebag",
                   trControl = train_control)
```

```{r}
treebag_predictions <- predict.train(treebag_fit, newdata = data_test)
RMSE(data_test$total_sales_2013, treebag_predictions)
```

# Model performance evaluation and final words  

## Performance evaulation  

## External validity?  

---
title: "Sales Prediction Exercise"
author: "Tamas Koncz"
date: '2018-03-04'
output:
  html_notebook:
    df_print: paged
    toc: true
    number_sections: true
    theme: united
  html_document:
    df_print: paged
    toc: true
    number_sections: true
    theme: united
---

```{r setup, message=FALSE, include=FALSE}
library(data.table)
library(dplyr)
library(lubridate)
library(ggplot2)
library(gridExtra)
library(scales)
library(reshape)

library(caret)
library(glmnet)

library(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

options(digits = 4)
options(scipen = 999)

theme_set(theme_minimal())   # globally set ggplot theme

set.seed(93)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
```

```{r, echo=FALSE}
fun_count_na <- function(dt) {
  ##counts and reports the missing observations for each column in a data.table object
  missing_values <- as.data.table(t(dt[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dt)]),
                                  keep.rownames = TRUE)
  setnames(missing_values, c("variable", "NA Count"))
  
  return(missing_values[order(-`NA Count`)])
}
```

# Purpose definition

The goal of this exercise is to predict sales for the coming year for different customers (groups of customers). I'll use standard prediction techniques and practices to create predictive models, while explaining the decision I've made along the code as well.  

The output will be available in an .Rmd and .html files.  
My documentation approach is the following: the .html output will show only limited amount of code, only snippets that are helpful for understanding the logic applied (others are "hidden" by the 'echo = FALSE' prefix for code chunks).  
For the full code base, please refer to the .Rmd file.  
  
The analysis is built of the following parts:  
    1. Data review & cleaning  
    2. Exploratory data analysis  
    3. Featuring engineering  
    4. Model & model selection
    5. Performance evaluation

# Data review & cleaning  

After reading in the data, let's take a glimpse at the data structure we'll be working with:
```{r, echo = FALSE}
data <- fread("./../Data/Sales Prediction/training.csv", stringsAsFactors = F)
glimpse(data)
head(data)
```  
  
One thing to notice is that we have a field containing dates, however R treats it as a character column by default. 
  
We'll correct that in the next step, but before that let's check if there is any observation that will require some fixing as well:

```{r, echo=FALSE}
fun_count_na(data)
```
  
No NAs, that's good news. Before deciding whether the dataset is complete or not, let's make sure all the sales & quantity values make sense, by checking how many 0-s we encounter.  
  
First, for quantities:  
```{r}
data %>%
  filter(quantity == 0) %>%
  summarize(count = n(),
            sum_sales_amount = sum(sales_amount))
```  
This looks very limited. Without further context on the data, I'll just assume they are recording errors, and exlcude them for the dataset.  

```{r}
data %>%
  filter(quantity != 0) %>%
  filter(sales_amount == 0) %>%
  summarize(count = n(),
            sum_quantity = sum(quantity))
```

We are encountering many (6947 in total) observations for which sales_amount is 0. Interestingly, sometimes even the record quantity is larger than 1 for these.   
Although they are marginal compared to the number of all observations (500K+), a decision needs to be still made how to handle them, as they are likely misleading for any model in their current format.  

```{r, include = FALSE}
data %>%
  filter(product_id == 1533517) %>%
  head()
```  
  
There might be a reason behind having 0 values (one I can think of is they are being part of some kind of promotion), and this reason could actually be indicative of future sales (e.g. the promotion is for frequent shoppers etc.).   
In this case, features should be created out of these items to help enhance the model's prediction.  
On the other hand, they might just be simple data issues. Deleted items still showing up as line items, incorrect prices (hence sales_amount), or anything similar.  
Right now we can simply and assume they actually represent wrong data - so by taking the easy path, I simply exclude them from further modeling efforts.  
  
Applying the fixes before we continue:  
```{r}
data <- data %>%
          filter(sales_amount > 0) %>% 
          mutate(purchase_date = as.Date(purchase_date, "%Y-%m-%d")) %>%
          arrange(contact_id, purchase_date) %>%
          select(contact_id, order_id, purchase_date, product_id, quantity, sales_amount)
```

# Data exploration  
  
As standard practice, I'll continue by visually exploring what's in the data, that might be useful for sales prediction.  
  
My starting hypothesis is that one of the most important features to understand is whether we are working returning customers - which materializes as having more than one purchase_date per contact_id (the other is the seasonality effect - more on that later).  
  
The below table gives us a brief look into customers who are generating the most number of transactions:
```{r, echo = FALSE}
per_contact_avgs <- data %>%
                      group_by(contact_id) %>%
                      summarize(total_line_items = n(),
                                transaction_count = n_distinct(order_id),
                                unique_products = n_distinct(product_id),
                                avg_quantity = mean(quantity),
                                median_quantity = median(quantity),
                                avg_sales_amount = mean(sales_amount),
                                median_sales_amount = median(sales_amount),
                                total_sales_amount = sum(sales_amount))
per_contact_avgs %>%
  arrange(desc(transaction_count)) %>%
  head(10)
```
  
Below is a visualization of the distribution of transactions among customers, broken into two groups, one having more than 25 transactions, and one having less.  
```{r, fig.width=8, fig.height=4, fig.align='center'}
##visualize
per_contact_avgs %>%
  mutate(transaction_category = ifelse(transaction_count < 25, 
                                       "Less than 25 transactions",
                                       "25+ transactions")) %>%
  mutate(transaction_category = factor(transaction_category, levels =
                                         c("Less than 25 transactions", "25+ transactions"))) %>%
  ggplot() + 
  geom_histogram(aes(x= transaction_count, fill = transaction_category)) +
  facet_wrap(~transaction_category, scales = "free") +
  scale_y_continuous(name="# of contact_id-s", labels = comma) +
  scale_x_continuous(name="Count of total transactions", labels = comma) +
  labs(title = "Grouping contact_id-s based on number of transactions")
```

What do we learn from this?  
    1. First, most customers only buy once. This is important for our modeling, as we won't know anything about them (priori to their first purchase) when we try to forecast their sales. Hence, all we'll be able to rely on is some generalizations made based on when they make their purchase.  
    2. Any way we slice the customers, transaction counts will be heavily skewed to the left - even more than for lognormal distributions.
    3. A very important thing that we _don't_ learn is how much sales these different groups generate.  
    
Maybe not suprisingly, total sales per contract_id do closely resemble a lognormal distribution:
```{r, echo=FALSE}
p1 <- per_contact_avgs %>%
  mutate(transaction_category = ifelse(transaction_count < 2, 
                                       "One-time customer",
                                       "Returning customer")) %>%
  mutate(transaction_category = factor(transaction_category, levels =
                                         c("One-time customer", "Returning customer"))) %>%
  ggplot() + 
  geom_histogram(aes(x= total_sales_amount)) +
  scale_y_continuous(trans='log10', labels = comma) 
+
  facet_wrap(~transaction_category, scales = "free")
  
## add avg. spending per customer
## tidy out visual

```


```{r}
#make this one chart with overlapping dist-s:
p2 <- per_contact_avgs %>%
  ggplot() + 
  geom_histogram(aes(x= log(avg_sales_amount))) +
  facet_wrap(~transaction_count < 2, scales = "free_y")

grid.arrange(p1, p2, ncol = 1)
```


## bit further text on this is needed... specially the avg. sales part


The second important factor to understand is how seasonality (basically time of purchase) is related to sales amounts.

```{r, echo = FALSE}
ordered_year_month <- data %>%
                        mutate(purchase_year = year(purchase_date)) %>%
                        mutate(purchase_month = month(purchase_date)) %>%
                        distinct(purchase_year, purchase_month) %>%
                        mutate(purchase_year_month = paste(as.character(purchase_year),
                                                           as.character(purchase_month), sep = "-")) %>%
                        arrange(purchase_year, purchase_month) %>%
                        select(purchase_year_month)

data <- data %>%
  mutate(purchase_year = year(purchase_date)) %>%
  mutate(purchase_month = month(purchase_date)) %>%
  mutate(purchase_year_month = paste(as.character(purchase_year),
                                     as.character(purchase_month), sep = "-")) %>%
  mutate(purchase_year_month = factor(purchase_year_month, 
                                      levels = ordered_year_month[["purchase_year_month"]])) 

```

```{r}
fun_plot_trend <- function(df1, x_var, y_var, x_breaks) {
  
  p <- ggplot(data = df1, aes_string(x= x_var)) + 
    geom_point(aes_string(y = y_var)) +
    geom_line(aes_string(y = y_var), group=1) +
    scale_x_discrete(breaks = x_breaks)  
    
  return(p)
}
```

```{r, echo = FALSE}
month_breaks <- ordered_year_month$purchase_year_month[seq(0, 24, 3)]

p1 <- data %>%
  group_by(purchase_year_month) %>%
  summarize(sales = sum(sales_amount)) %>%
  fun_plot_trend(x_var = "purchase_year_month", 
                 y_var = "sales", x_breaks = month_breaks)
  
  
p2 <- data %>%
  group_by(purchase_year_month) %>%
  summarize(transaction_count = n_distinct(order_id)) %>%
  fun_plot_trend(x_var = "purchase_year_month", 
                 y_var = "transaction_count", x_breaks = month_breaks)

p3 <- data %>%
  group_by(purchase_year_month, order_id) %>%
  summarize(sales_amount = sum(sales_amount)) %>%
  group_by(purchase_year_month) %>%
  summarize(avg_sales_amount = mean(sales_amount)) %>%
  fun_plot_trend(x_var = "purchase_year_month", 
                 y_var = "avg_sales_amount", x_breaks = month_breaks)

grid.arrange(p1, p2, p3, ncol = 1)

##todo tidy-up, explain
```

```{r}
data %>%
  arrange(contact_id, purchase_date) %>%
  head(20)
```
```{r}
#TODO: break this out by purchase count 0 or 1+ by contact_id
data %>%
  group_by(purchase_year, purchase_month, contact_id, order_id) %>%
  summarize(sales_amount = sum(sales_amount)) %>%
  group_by(purchase_year, purchase_month, contact_id) %>%
  summarize(avg_sales_amount = mean(sales_amount)) %>%
  group_by(purchase_year, purchase_month) %>%
  summarize(avg_sales_amount = mean(avg_sales_amount)) %>%
  ggplot(aes(x = as.factor(purchase_month), y = avg_sales_amount)) +
    geom_bar(stat="identity") + 
    scale_x_discrete(breaks = c(1:12)) +
    facet_grid(~purchase_year)
```

# Feature engineering

# Modeling & model selection

# Model performance evaluation and final words
